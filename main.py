# main.py (PDF ë° ì´ë¯¸ì§€ ì²˜ë¦¬ 0718 ì˜¤í›„ 12ì‹œ 28ë¶„ )

import asyncio
import os
import fitz  # PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
import re
import shutil

from pathlib import Path
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from PIL import Image # Pillow ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import io # ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ë‹¤ë£¨ê¸° ìœ„í•œ import

import google.generativeai as genai
from google.cloud import speech, texttospeech

from fastapi import Body

# --- 1. ì„¤ì • ---
KNOWLEDGE_PDF_PATH = "knowledge.pdf"
IMAGES_DIR = Path(__file__).resolve().parent / "static" / "images"

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
MAX_OUTPUT_TOKENS = 1500
STT_CREDENTIALS_PATH = "/etc/secrets/voice-chat-462608-412b0459f610.json"
TTS_CREDENTIALS_PATH = "/etc/secrets/voice-chat-462608-e445e48514e2.json"
SAMPLE_RATE = 48000 # ë¸Œë¼ìš°ì € MediaRecorder ê¸°ë³¸ê°’

# --- 2. ì•± ë° ì „ì—­ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
app = FastAPI()
MODEL, STT_CLIENT, TTS_CLIENT, KNOWLEDGE_CONTEXT = None, None, None, None
INITIALIZATION_ERROR = None

PDF_CONTENT = [] 

@app.on_event("startup")
def startup_event():
    """ì„œë²„ê°€ ì‹œì‘ë  ë•Œ ëª¨ë“  API í´ë¼ì´ì–¸íŠ¸ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¯¸ë¦¬ ë¡œë”©í•©ë‹ˆë‹¤."""
    global MODEL, STT_CLIENT, TTS_CLIENT, PDF_CONTENT, INITIALIZATION_ERROR
    try:
        print("âœ¨  ì•± ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” (PDF ì²˜ë¦¬ í¬í•¨)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        if IMAGES_DIR.exists():
            print(f"ğŸ“ ê¸°ì¡´ ì´ë¯¸ì§€ í´ë”({IMAGES_DIR})ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...")
            shutil.rmtree(IMAGES_DIR)
        print(f"ğŸ“ ìƒˆë¡œìš´ ì´ë¯¸ì§€ í´ë”({IMAGES_DIR})ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        IMAGES_DIR.mkdir()

        pdf_path = Path(__file__).resolve().parent / KNOWLEDGE_PDF_PATH
        if not pdf_path.exists(): raise FileNotFoundError(f"{pdf_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        doc = fitz.open(pdf_path)
#        extracted_content = []
        for page_num, page in enumerate(doc):
            page_data = {"page": page_num, "text": page.get_text()}
            image_files = []

            for img_index, img in enumerate(page.get_images(full=True)):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]

            # --- âœ¨ Pillowë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë³€í™˜ ë¡œì§ (í•µì‹¬ ìˆ˜ì •) âœ¨ ---
                try:
                    # 1. ì¶”ì¶œí•œ ì´ë¯¸ì§€ ë°”ì´íŠ¸ë¥¼ Pillow ì´ë¯¸ì§€ ê°ì²´ë¡œ ë³€í™˜
                    image_obj = Image.open(io.BytesIO(image_bytes))
                    # 2. íŒŒì¼ ì´ë¦„ì„ PNGë¡œ ê³ ì •
                    image_filename = f"page_{page_num}_img_{img_index}.png"
     #               save_path = IMAGES_DIR / image_filename
                    # 3. PNG í¬ë§·ìœ¼ë¡œ íŒŒì¼ ì €ì¥ (ì´ ê³¼ì •ì—ì„œ ë³€í™˜ì´ ì¼ì–´ë‚¨)
                    image_obj.save(IMAGES_DIR / image_filename, "PNG")
                    image_files.append(image_filename)
                except Exception as img_e:
                    print(f"ê²½ê³ : ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ (í˜ì´ì§€ {page_num}): {img_e}")

                # --- âœ¨ ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • ---


                page_data["images"] = image_files
                PDF_CONTENT.append(page_data)
                print(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ: {len(doc)} í˜ì´ì§€, {sum(len(p['images']) for p in PDF_CONTENT)}ê°œ ì´ë¯¸ì§€ ì¶”ì¶œ")


        # 2. í´ë¼ì´ì–¸íŠ¸ ë° ëª¨ë¸ ì´ˆê¸°í™” (ì‹œìŠ¤í…œ ëª…ë ¹ì–´ ìˆ˜ì •)
        STT_CLIENT = speech.SpeechClient.from_service_account_file(STT_CREDENTIALS_PATH)
        TTS_CLIENT = texttospeech.TextToSpeechClient.from_service_account_file(TTS_CREDENTIALS_PATH)
        genai.configure(api_key=GEMINI_API_KEY)
        
        system_instruction = f"""
            ë‹¹ì‹ ì€ ì „ë¬¸ Q&A ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
            ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´, ì œê³µë˜ëŠ” ì»¨í…ìŠ¤íŠ¸(í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€)ë§Œì„ ì‚¬ìš©í•˜ì—¬ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
            ë§Œì•½ ë‹µë³€ì´ ì œê³µëœ íŠ¹ì • ì´ë¯¸ì§€ì™€ ê´€ë ¨ì´ ìˆë‹¤ë©´, ë°˜ë“œì‹œ ë‹µë³€ì˜ ë§¨ ëì— ë‹¤ìŒ í˜•ì‹ì˜ íƒœê·¸ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤: [IMAGE: ì´ë¯¸ì§€íŒŒì¼ì´ë¦„.png]
            ë‹¹ì‹ ì˜ ë‚´ë¶€ ì§€ì‹ì´ë‚˜ ë‹¤ë¥¸ ì •ë³´ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
        
            âœ¨âœ¨âœ¨ [ì¤‘ìš” ê·œì¹™] âœ¨âœ¨âœ¨
            1. ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ 2000ì ì´ë‚´ë¡œ, í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì„œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
            2. ë‹µë³€ì´ ê¸¸ì–´ì§ˆ ê²½ìš°, ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë¶€í„° ìˆœì„œëŒ€ë¡œ, ìµœëŒ€ 3~4ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
            3. ì¹œì ˆí•˜ê³  ëª…í™•í•œ í•œêµ­ì–´ ë§íˆ¬ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.

           """

        generation_config = genai.GenerationConfig(max_output_tokens=MAX_OUTPUT_TOKENS)
        MODEL = genai.GenerativeModel('gemini-1.5-flash', system_instruction=system_instruction, generation_config=generation_config)
        
        print("ğŸ‰ ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ. ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        INITIALIZATION_ERROR = f"[{type(e).__name__}] {e}"
        print(f"ğŸ’¥ FATAL: ì•± ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ! ì›ì¸: {INITIALIZATION_ERROR}")
        
# --- 3. FastAPI ì—”ë“œí¬ì¸íŠ¸ ---
BASE_DIR = Path(__file__).resolve().parent
app.mount("/static", StaticFiles(directory=BASE_DIR / "static"), name="static")
@app.get("/", response_class=FileResponse)
async def read_index(): return FileResponse(BASE_DIR / "static" / "index.html")


# âœ¨âœ¨âœ¨ AR ë„ìŠ¨íŠ¸ ìŒì„± ì•ˆë‚´ë¥¼ ìœ„í•œ TTS API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ âœ¨âœ¨âœ¨
@app.post("/api/tts")
async def text_to_speech_api(text_to_speak: str = Body(..., embed=True)):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±(MP3) ë°ì´í„°ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not TTS_CLIENT:
        return {"error": "TTS client not initialized"}
    
    try:
        input_text = texttospeech.SynthesisInput(text=text_to_speak)
        voice = texttospeech.VoiceSelectionParams(language_code="ko-KR", name="ko-KR-Wavenet-A")
        audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)
        tts_request = texttospeech.SynthesizeSpeechRequest(input=input_text, voice=voice, audio_config=audio_config)
        
        tts_response = await asyncio.to_thread(TTS_CLIENT.synthesize_speech, request=tts_request)
        
        # Base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜
        import base64
        audio_base64 = base64.b64encode(tts_response.audio_content).decode('utf-8')
        return {"audio": audio_base64}
        
    except Exception as e:
        print(f"ğŸ’¥ TTS API ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

# âœ¨âœ¨âœ¨ PDF ë‚´ìš©ì„ í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•œ API ì¶”ê°€ âœ¨âœ¨âœ¨
@app.get("/api/pdf-content")
async def get_pdf_content():
    """ì¶”ì¶œëœ PDFì˜ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if PDF_CONTENT is None:
        return {"error": "PDF content not loaded"}
    return PDF_CONTENT




@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    if INITIALIZATION_ERROR:
        await websocket.send_json({"type": "error", "data": f"ì„œë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {INITIALIZATION_ERROR}"})
        await websocket.close(); return
            
    client_id = f"{websocket.client.host}:{websocket.client.port}"
    print(f"âœ… í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨: {client_id}")
    try:
        while True:
            raw_data = await websocket.receive_json()
            message_type = raw_data.get("type")
            user_input = raw_data.get("data")
            user_text = ""

            if message_type == "audio":
                import base64
                audio_bytes = base64.b64decode(user_input)
                config = speech.RecognitionConfig(encoding=speech.RecognitionConfig.AudioEncoding.WEBM_OPUS, sample_rate_hertz=SAMPLE_RATE, language_code="ko-KR")
                audio = speech.RecognitionAudio(content=audio_bytes)
                stt_response = await asyncio.to_thread(STT_CLIENT.recognize, request={"config": config, "audio": audio})
                user_text = stt_response.results[0].alternatives[0].transcript if stt_response.results else ""
                if user_text: await websocket.send_json({"type": "user_text", "data": user_text})
            elif message_type == "text":
                user_text = user_input

            if user_text:
                if "ì´ì œ ê·¸ë§Œ" in user_text.strip(): 
                    ai_text = "ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤."
                    await websocket.send_json({"type": "ai_text", "data": ai_text})
                    break

                keywords = user_text.split()
                relevant_pages = [
                    page for page in PDF_CONTENT 
                    if any(keyword.lower() in page["text"].lower() for keyword in keywords if len(keyword) > 1)
                ]
                if not relevant_pages: # ê´€ë ¨ëœ í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                    relevant_pages = PDF_CONTENT

                # --- âœ¨ 2. Geminiì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„± (í…ìŠ¤íŠ¸ + 'íŒŒì¼ëª…' + ì´ë¯¸ì§€) âœ¨ ---
                prompt_parts = []
                prompt_parts.append("ì•„ë˜ ì œê³µëœ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜. ì´ë¯¸ì§€ì— ëŒ€í•´ ì–¸ê¸‰í•  ë•ŒëŠ”, ë‚´ê°€ ì•Œë ¤ì¤€ 'ì´ë¯¸ì§€ íŒŒì¼ëª…'ì„ ì‚¬ìš©í•˜ì—¬ `[IMAGE: íŒŒì¼ëª…]` íƒœê·¸ë¥¼ ë¶™ì—¬ì•¼ í•´.")
                
                for page in relevant_pages[:3]: # ë„ˆë¬´ ë§ì€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ë‚´ì§€ ì•Šë„ë¡ 3í˜ì´ì§€ë§Œ ì°¸ê³ 
                    prompt_parts.append(f"\n--- ì°¸ê³  í…ìŠ¤íŠ¸ (í˜ì´ì§€ {page['page']+1}) ---\n{page['text']}")
                    if page["images"]:
                        prompt_parts.append("\n--- ì´ í˜ì´ì§€ì˜ ì°¸ê³  ì´ë¯¸ì§€ ---")
                        for img_file in page["images"]:
                            img_path = IMAGES_DIR / img_file
                            if img_path.exists():
                                try:
                                    prompt_parts.append(Image.open(img_path))
                                    # âœ¨âœ¨âœ¨ ë°”ë¡œ ì´ ë¶€ë¶„ì˜ ëª…ë ¹ì–´ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤ âœ¨âœ¨âœ¨
                                    prompt_parts.append(f"ì°¸ê³ : ìœ„ ì´ë¯¸ì§€ì˜ íŒŒì¼ëª…ì€ '{img_file}'ì…ë‹ˆë‹¤. ë‹µë³€ ë³¸ë¬¸ì—ëŠ” ì´ íŒŒì¼ëª…ì„ ì ˆëŒ€ ì§ì ‘ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ì´ë¯¸ì§€ë¥¼ ì°¸ê³ í–ˆì„ ê²½ìš°ì—ë§Œ ë‹µë³€ ë§ˆì§€ë§‰ì— `[IMAGE: {img_file}]` íƒœê·¸ë§Œ ì¶”ê°€í•´ì£¼ì„¸ìš”.")

                                except Exception as img_e:
                                    print(f"ê²½ê³ : ì´ë¯¸ì§€ íŒŒì¼ì„ ì—¬ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ {img_path}: {img_e}")
                
                prompt_parts.append(f"\n--- ì§ˆë¬¸ ---\n{user_text}")

                # --- âœ¨ 3. ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ë¡œ Gemini í˜¸ì¶œ âœ¨ ---
                gemini_response = await MODEL.generate_content_async(prompt_parts)
                ai_text_raw = gemini_response.text

                # --- âœ¨ 4. ë‹µë³€ì—ì„œ ì´ë¯¸ì§€ íƒœê·¸ íŒŒì‹± ë° ë¶„ë¦¬ ì „ì†¡ âœ¨ ---
                image_tag_match = re.search(r"\[IMAGE:\s*(page_\d+_img_\d+\.\w+)\]", ai_text_raw)
                ai_text = re.sub(r"\[IMAGE:\s*(.*?)\]", "", ai_text_raw).strip()

                await websocket.send_json({"type": "ai_text", "data": ai_text})

                if image_tag_match:
                    image_filename = image_tag_match.group(1).strip()
                    image_url = f"/static/images/{image_filename}"
                    print(f"ğŸ–¼ï¸ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì´ë¯¸ì§€ í‘œì‹œ ìš”ì²­: {image_url}")
                    await websocket.send_json({"type": "ai_image", "url": image_url})

             
                tts_request = texttospeech.SynthesizeSpeechRequest(input=texttospeech.SynthesisInput(text=ai_text), voice=texttospeech.VoiceSelectionParams(language_code="ko-KR", name="ko-KR-Wavenet-A"), audio_config=texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3))
                tts_response = await asyncio.to_thread(TTS_CLIENT.synthesize_speech, request=tts_request)
                if tts_response.audio_content: await websocket.send_bytes(tts_response.audio_content)
                if "ì´ì œ ê·¸ë§Œ" in user_text.strip(): await asyncio.sleep(1); break

    except WebSocketDisconnect: print(f"ğŸ”Œ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠì–´ì§: {client_id}")
    except Exception as e: print(f"ğŸ’¥ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({client_id}): {e}")
    finally: print(f"ğŸ ì›¹ì†Œì¼“ ì„¸ì…˜ ì¢…ë£Œ: {client_id}")
